{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "180feece-fe56-4ec3-8e3d-bc8305b6f24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/evaedwards/Final-Year-Project/Datasets/TXT\n",
      "Merged CSV file saved as 'combined_regions61.csv'\n",
      "Saved filtered data for matc2 to CP034461_txt/MATC2_CH61.txt\n",
      "Saved filtered data for WS4 to CP034461_txt/WS4_CH61.txt\n",
      "Saved filtered data for jog19 to CP034461_txt/JOG19_CH61.txt\n",
      "Saved filtered data for WS5 to CP034461_txt/WS5_CH61.txt\n",
      "Saved filtered data for matc3 to CP034461_txt/MATC3_CH61.txt\n",
      "Saved filtered data for matc1 to CP034461_txt/MATC1_CH61.txt\n",
      "Saved filtered data for WS7 to CP034461_txt/WS7_CH61.txt\n",
      "Saved filtered data for WS6 to CP034461_txt/WS6_CH61.txt\n",
      "Saved filtered data for matc4 to CP034461_txt/MATC4_CH61.txt\n",
      "Saved filtered data for WS2 to CP034461_txt/WS2_CH61.txt\n",
      "Saved filtered data for WS3 to CP034461_txt/WS3_CH61.txt\n",
      "Saved filtered data for WS1 to CP034461_txt/WS1_CH61.txt\n",
      "Saved filtered data for jog20 to CP034461_txt/JOG20_CH61.txt\n",
      "Saved filtered data for jog21 to CP034461_txt/JOG21_CH61.txt\n",
      "Saved filtered data for mat22 to CP034461_txt/MAT22_CH61.txt\n",
      "Saved filtered data for PP16 to CP034461_txt/PP16_CH61.txt\n",
      "Saved filtered data for jog1 to CP034461_txt/JOG1_CH61.txt\n",
      "Saved filtered data for jogc5 to CP034461_txt/JOGC5_CH61.txt\n",
      "Saved filtered data for jogc4 to CP034461_txt/JOGC4_CH61.txt\n",
      "Saved filtered data for PP17 to CP034461_txt/PP17_CH61.txt\n",
      "Saved filtered data for mat23 to CP034461_txt/MAT23_CH61.txt\n",
      "Saved filtered data for mat21 to CP034461_txt/MAT21_CH61.txt\n",
      "Saved filtered data for PP15 to CP034461_txt/PP15_CH61.txt\n",
      "Saved filtered data for jog2 to CP034461_txt/JOG2_CH61.txt\n",
      "Saved filtered data for NW13 to CP034461_txt/NW13_CH61.txt\n",
      "Saved filtered data for jog3 to CP034461_txt/JOG3_CH61.txt\n",
      "Saved filtered data for PP14 to CP034461_txt/PP14_CH61.txt\n",
      "Saved filtered data for APC12 to CP034461_txt/APC12_CH61.txt\n",
      "Saved filtered data for mat20 to CP034461_txt/MAT20_CH61.txt\n",
      "Saved filtered data for mat24 to CP034461_txt/MAT24_CH61.txt\n",
      "Saved filtered data for mat18 to CP034461_txt/MAT18_CH61.txt\n",
      "Saved filtered data for PP10 to CP034461_txt/PP10_CH61.txt\n",
      "Saved filtered data for jog7 to CP034461_txt/JOG7_CH61.txt\n",
      "Saved filtered data for NW16 to CP034461_txt/NW16_CH61.txt\n",
      "Saved filtered data for NW17 to CP034461_txt/NW17_CH61.txt\n",
      "Saved filtered data for jogc2 to CP034461_txt/JOGC2_CH61.txt\n",
      "Saved filtered data for jog6 to CP034461_txt/JOG6_CH61.txt\n",
      "Saved filtered data for PP11 to CP034461_txt/PP11_CH61.txt\n",
      "Saved filtered data for mat19 to CP034461_txt/MAT19_CH61.txt\n",
      "Saved filtered data for PP8 to CP034461_txt/PP8_CH61.txt\n",
      "Saved filtered data for PP13 to CP034461_txt/PP13_CH61.txt\n",
      "Saved filtered data for NW15 to CP034461_txt/NW15_CH61.txt\n",
      "Saved filtered data for NW29 to CP034461_txt/NW29_CH61.txt\n",
      "Saved filtered data for NW28 to CP034461_txt/NW28_CH61.txt\n",
      "Saved filtered data for jogc1 to CP034461_txt/JOGC1_CH61.txt\n",
      "Saved filtered data for NW14 to CP034461_txt/NW14_CH61.txt\n",
      "Saved filtered data for jog5 to CP034461_txt/JOG5_CH61.txt\n",
      "Saved filtered data for PP12 to CP034461_txt/PP12_CH61.txt\n",
      "Saved filtered data for PP9 to CP034461_txt/PP9_CH61.txt\n",
      "Saved filtered data for mat17 to CP034461_txt/MAT17_CH61.txt\n",
      "Saved filtered data for PP4 to CP034461_txt/PP4_CH61.txt\n",
      "Saved filtered data for PP23 to CP034461_txt/PP23_CH61.txt\n",
      "Saved filtered data for jog8 to CP034461_txt/JOG8_CH61.txt\n",
      "Saved filtered data for NW19 to CP034461_txt/NW19_CH61.txt\n",
      "Saved filtered data for NW31 to CP034461_txt/NW31_CH61.txt\n",
      "Saved filtered data for NW25 to CP034461_txt/NW25_CH61.txt\n",
      "Saved filtered data for NW24 to CP034461_txt/NW24_CH61.txt\n",
      "Saved filtered data for NW30 to CP034461_txt/NW30_CH61.txt\n",
      "Saved filtered data for NW18 to CP034461_txt/NW18_CH61.txt\n",
      "Saved filtered data for PP22 to CP034461_txt/PP22_CH61.txt\n",
      "Saved filtered data for PP5 to CP034461_txt/PP5_CH61.txt\n",
      "Saved filtered data for mat16 to CP034461_txt/MAT16_CH61.txt\n",
      "Saved filtered data for mat14 to CP034461_txt/MAT14_CH61.txt\n",
      "Saved filtered data for PP7 to CP034461_txt/PP7_CH61.txt\n",
      "Saved filtered data for PP20 to CP034461_txt/PP20_CH61.txt\n",
      "Saved filtered data for NW26 to CP034461_txt/NW26_CH61.txt\n",
      "Saved filtered data for NW32 to CP034461_txt/NW32_CH61.txt\n",
      "Saved filtered data for NW33 to CP034461_txt/NW33_CH61.txt\n",
      "Saved filtered data for NW27 to CP034461_txt/NW27_CH61.txt\n",
      "Saved filtered data for PP21 to CP034461_txt/PP21_CH61.txt\n",
      "Saved filtered data for PP6 to CP034461_txt/PP6_CH61.txt\n",
      "Saved filtered data for mat15 to CP034461_txt/MAT15_CH61.txt\n",
      "Saved filtered data for PP25 to CP034461_txt/PP25_CH61.txt\n",
      "Saved filtered data for PP19 to CP034461_txt/PP19_CH61.txt\n",
      "Saved filtered data for NW23 to CP034461_txt/NW23_CH61.txt\n",
      "Saved filtered data for NW36 to CP034461_txt/NW36_CH61.txt\n",
      "Saved filtered data for NW22 to CP034461_txt/NW22_CH61.txt\n",
      "Saved filtered data for PP18 to CP034461_txt/PP18_CH61.txt\n",
      "Saved filtered data for PP3 to CP034461_txt/PP3_CH61.txt\n",
      "Saved filtered data for PP24 to CP034461_txt/PP24_CH61.txt\n",
      "Saved filtered data for PP1 to CP034461_txt/PP1_CH61.txt\n",
      "Saved filtered data for NW34 to CP034461_txt/NW34_CH61.txt\n",
      "Saved filtered data for NW20 to CP034461_txt/NW20_CH61.txt\n",
      "Saved filtered data for NW21 to CP034461_txt/NW21_CH61.txt\n",
      "Saved filtered data for NW35 to CP034461_txt/NW35_CH61.txt\n",
      "Saved filtered data for mat13 to CP034461_txt/MAT13_CH61.txt\n",
      "Saved filtered data for jog10 to CP034461_txt/JOG10_CH61.txt\n",
      "Saved filtered data for jog11 to CP034461_txt/JOG11_CH61.txt\n",
      "Saved filtered data for jog13 to CP034461_txt/JOG13_CH61.txt\n",
      "Saved filtered data for jog12 to CP034461_txt/JOG12_CH61.txt\n",
      "Saved filtered data for WS11 to CP034461_txt/WS11_CH61.txt\n",
      "Saved filtered data for WS10 to CP034461_txt/WS10_CH61.txt\n",
      "Saved filtered data for WS8 to CP034461_txt/WS8_CH61.txt\n",
      "Saved filtered data for WS12 to CP034461_txt/WS12_CH61.txt\n",
      "Saved filtered data for jog15 to CP034461_txt/JOG15_CH61.txt\n",
      "Saved filtered data for jog14 to CP034461_txt/JOG14_CH61.txt\n",
      "Saved filtered data for WS9 to CP034461_txt/WS9_CH61.txt\n"
     ]
    }
   ],
   "source": [
    "# This code will create a table for each chromosome showing the\n",
    "# normalized read depth across teh chromosome annoated back to the region\n",
    "\n",
    "# Step 1: Check working directory\n",
    "import os\n",
    "os.chdir('/Users/evaedwards/Final-Year-Project/Datasets/TXT')\n",
    "print(os.getcwd())\n",
    "\n",
    "# Step 2: Make txt. file for the chromosome i.e CP034462\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define input and output file pathsfrom Bio import SeqIO\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "genbank_file = \"GCA_004217705.1_ASM421770v1_genomic-1.gbk\"\n",
    "record_id = \"CP034462.1\"  # Change this to the appropriate chromosome ID\n",
    "\n",
    "# Parse the GenBank file and find the correct record\n",
    "record = None\n",
    "for seq_record in SeqIO.parse(genbank_file, \"genbank\"):\n",
    "    if seq_record.id == record_id:\n",
    "        record = seq_record\n",
    "        break\n",
    "\n",
    "if record is None:\n",
    "    raise ValueError(f\"Record ID '{record_id}' not found in {genbank_file}\")\n",
    "\n",
    "# Initialize lists to store genic and intergenic region data\n",
    "genic_regions = []\n",
    "intergenic_regions = []\n",
    "\n",
    "# Extract genic regions (prefer 'gene' over 'CDS')\n",
    "seen_genes = set()\n",
    "for feature in record.features:\n",
    "    if feature.type in [\"gene\", \"CDS\"]:  # Adjust as needed\n",
    "        start = feature.location.start\n",
    "        end = feature.location.end\n",
    "        gene_name = feature.qualifiers.get(\"gene\", [\"Unknown\"])[0]  # Get gene name if available\n",
    "        product = feature.qualifiers.get(\"product\", [\"unknown\"])[0]\n",
    "        \n",
    "        # Avoid duplicates by using (start, end) as a unique key\n",
    "        if (start, end) not in seen_genes:\n",
    "            seen_genes.add((start, end))\n",
    "            genic_regions.append((start, end, gene_name, product))\n",
    "\n",
    "# Sort genic regions by start position\n",
    "genic_regions.sort()\n",
    "\n",
    "# Extract intergenic regions\n",
    "prev_end = 0  # Start of chromosome\n",
    "intergenic_names = []\n",
    "\n",
    "for i in range(len(genic_regions)):\n",
    "    start, end, gene_name, _ = genic_regions[i]\n",
    "\n",
    "    # If there's a gap, define an intergenic region\n",
    "    if prev_end < start:\n",
    "        # Get flanking gene names\n",
    "        upstream_gene = genic_regions[i - 1][2] if i > 0 else \"-\"\n",
    "        downstream_gene = gene_name\n",
    "        intergenic_name = f\"{upstream_gene} / {downstream_gene}\"\n",
    "        intergenic_regions.append((prev_end, start, start - prev_end, intergenic_name))\n",
    "    \n",
    "    prev_end = end  # Move to the end of current gene\n",
    "\n",
    "# Capture last intergenic region (if any)\n",
    "if prev_end < len(record.seq):\n",
    "    last_gene = genic_regions[-1][2] if genic_regions else \"-\"\n",
    "    intergenic_name = f\"{last_gene} / -\"\n",
    "    intergenic_regions.append((prev_end, len(record.seq), len(record.seq) - prev_end, intergenic_name))\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "genic_df = pd.DataFrame(genic_regions, columns=[\"Start\", \"End\", \"Gene\", \"Product\"])\n",
    "intergenic_df = pd.DataFrame(intergenic_regions, columns=[\"Start\", \"End\", \"Length\", \"Gene\"])\n",
    "\n",
    "# Add a column to differentiate genic vs. intergenic\n",
    "genic_df[\"Type\"] = \"genic\"\n",
    "intergenic_df[\"Type\"] = \"intergenic\"\n",
    "\n",
    "# Merge both DataFrames\n",
    "combined_df = pd.concat([genic_df, intergenic_df], ignore_index=True)\n",
    "\n",
    "# Sort by genomic position\n",
    "combined_df = combined_df.sort_values(by=\"Start\").reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "combined_df.to_csv(\"combined_regions62.csv\", index=False)\n",
    "\n",
    "print(\"Merged CSV file saved as 'combined_regions62.csv'\")\n",
    "\n",
    "input_files = glob.glob(\"TXT/*.txt\")  # Adjust to match your directory\n",
    "output_folder = \"CP034462_txt/\"  # Folder to store filtered files\n",
    "os.makedirs(output_folder, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "# Process each file\n",
    "for file in input_files:\n",
    "    strain_name = os.path.basename(file).replace(\".txt\", \"\")  # Extract strain name\n",
    "    \n",
    "    # Load the coverage data (assuming tab-separated format, no headers)\n",
    "    df = pd.read_csv(file, sep=\"\\t\", header=None, names=[\"Chromosome\", \"Position\", \"Coverage\"])\n",
    "    \n",
    "    # Filter only for the chromosome i.e CP034462\n",
    "    df_filtered = df[(df[\"Chromosome\"] == \"CP034462\")]\n",
    "    \n",
    "    # Save the filtered data - change name for the chromosome i.e CP034462\n",
    "    output_file = os.path.join(output_folder, f\"{strain_name.upper()}_CH62.txt\")\n",
    "    df_filtered.to_csv(output_file, sep=\"\\t\", index=False, header=False)\n",
    "    \n",
    "    print(f\"Saved filtered data for {strain_name} to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1a6b25d6-106f-410c-9a02-854bf407a7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV file saved as 'combined_regions_with_product62.csv'\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "genbank_file = \"GCA_004217705.1_ASM421770v1_genomic-1.gbk\"\n",
    "record_id = \"CP034462.1\"  # Change this to the appropriate chromosome ID\n",
    "\n",
    "# Initialize lists to store genic and intergenic region data\n",
    "genic_regions = []\n",
    "intergenic_regions = []\n",
    "\n",
    "# Open the GenBank file\n",
    "with open(genbank_file, \"r\") as handle:\n",
    "    for record in SeqIO.parse(handle, \"genbank\"):\n",
    "        if record.id == record_id:\n",
    "            seen_genes = set()\n",
    "\n",
    "            # Extract genic regions (specifically for CDS and gene features)\n",
    "            for feature in record.features:\n",
    "                if feature.type == \"CDS\":  # Focus on coding sequences\n",
    "                    start = feature.location.start\n",
    "                    end = feature.location.end\n",
    "                    \n",
    "                    # Get gene name and product (or 'unknown' if not available)\n",
    "                    gene_name = feature.qualifiers.get(\"gene\", [\"unknown\"])[0]\n",
    "                    product = feature.qualifiers.get(\"product\", [\"unknown\"])[0]\n",
    "                    \n",
    "                    # Avoid duplicates by using (start, end) as a unique key\n",
    "                    if (start, end) not in seen_genes:\n",
    "                        seen_genes.add((start, end))\n",
    "                        genic_regions.append((start, end, gene_name, product))\n",
    "\n",
    "            # Sort genic regions by start position\n",
    "            genic_regions.sort()\n",
    "\n",
    "            # Extract intergenic regions\n",
    "            prev_end = 0  # Start of chromosome\n",
    "            for i in range(len(genic_regions)):\n",
    "                start, end, gene_name, product = genic_regions[i]\n",
    "\n",
    "                # If there's a gap, define an intergenic region\n",
    "                if prev_end < start:\n",
    "                    # Get flanking gene names and product\n",
    "                    upstream_gene = genic_regions[i - 1][2] if i > 0 else \"-\"\n",
    "                    downstream_gene = gene_name\n",
    "                    upstream_product = genic_regions[i - 1][3] if i > 0 else \"-\"\n",
    "                    downstream_product = product\n",
    "                    intergenic_name = f\"{upstream_gene} / {downstream_gene}\"\n",
    "                    intergenic_product = f\"{upstream_product} / {downstream_product}\"\n",
    "\n",
    "                    intergenic_regions.append((prev_end, start, start - prev_end, intergenic_name, intergenic_product))\n",
    "                \n",
    "                prev_end = end  # Move to the end of current gene\n",
    "\n",
    "            # Capture last intergenic region (if any)\n",
    "            if prev_end < len(record.seq):\n",
    "                last_gene = genic_regions[-1][2] if genic_regions else \"-\"\n",
    "                last_product = genic_regions[-1][3] if genic_regions else \"-\"\n",
    "                intergenic_name = f\"{last_gene} / -\"\n",
    "                intergenic_product = f\"{last_product} / -\"\n",
    "                intergenic_regions.append((prev_end, len(record.seq), len(record.seq) - prev_end, intergenic_name, intergenic_product))\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "genic_df = pd.DataFrame(genic_regions, columns=[\"Start\", \"End\", \"Gene\", \"Product\"])\n",
    "intergenic_df = pd.DataFrame(intergenic_regions, columns=[\"Start\", \"End\", \"Length\", \"Gene\", \"Product\"])\n",
    "\n",
    "# Add a column to differentiate genic vs. intergenic\n",
    "genic_df[\"Type\"] = \"genic\"\n",
    "intergenic_df[\"Type\"] = \"intergenic\"\n",
    "\n",
    "# Merge both DataFrames\n",
    "combined_df = pd.concat([genic_df, intergenic_df], ignore_index=True)\n",
    "\n",
    "# Sort by genomic position\n",
    "combined_df = combined_df.sort_values(by=\"Start\").reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "combined_df.to_csv(\"combined_regions_with_product62.csv\", index=False)\n",
    "\n",
    "print(\"Merged CSV file saved as 'combined_regions_with_product62.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "65e62313-95c3-4cee-bca6-655b58bc4b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/evaedwards/Final-Year-Project/Datasets/TXT\n",
      "                     Gene ID  \\\n",
      "0            - / MPUL0G00100   \n",
      "1                MPUL0G00100   \n",
      "2  MPUL0G00100 / MPUL0G00110   \n",
      "3                MPUL0G00110   \n",
      "4  MPUL0G00110 / MPUL0G00120   \n",
      "\n",
      "                                       Gene Function  Start Position  \\\n",
      "0  - / Transcriptional activator of glycolytic en...               0   \n",
      "1    Transcriptional activator of glycolytic enzymes            6215   \n",
      "2  Transcriptional activator of glycolytic enzyme...            8981   \n",
      "3    Transcriptional activator of glycolytic enzymes           10919   \n",
      "4  Transcriptional activator of glycolytic enzyme...           13685   \n",
      "\n",
      "   End Position  Length   APC12    JOG1  JOG10  JOG11  JOG12  ...    WS11  \\\n",
      "0          6215    6215   9.746   9.758  6.883  6.556  6.905  ...   9.707   \n",
      "1          8981    2766  10.034  10.159  7.255  6.894  7.267  ...  10.445   \n",
      "2         10919    1938  10.203  10.176  7.173  6.908  7.155  ...  10.183   \n",
      "3         13685    2766  10.034  10.159  7.255  6.894  7.267  ...  10.445   \n",
      "4         29471   15786   1.017   1.000  1.020  1.005  1.034  ...   1.012   \n",
      "\n",
      "     WS12    WS2    WS3     WS4     WS5     WS6     WS7     WS8    WS9  \n",
      "0   9.962  8.618  8.201   9.744   9.976  10.267  11.795   9.641  7.086  \n",
      "1  10.521  8.950  8.866  10.444  10.621  11.059  12.557  10.231  7.519  \n",
      "2  10.427  9.015  8.604  10.328  10.232  10.460  12.454  10.288  7.432  \n",
      "3  10.521  8.950  8.866  10.444  10.621  11.059  12.557  10.231  7.519  \n",
      "4   1.009  1.015  1.007   1.011   1.019   1.010   1.005   1.038  1.038  \n",
      "\n",
      "[5 rows x 102 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge with txt data\n",
    "\n",
    "# Step 4: \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# Define paths\n",
    "base_dir = \"/Users/evaedwards/Final-Year-Project/Datasets/TXT\"\n",
    "annotated_file = os.path.join(base_dir, \"combined_regions_with_product62.csv\")\n",
    "coverage_files = glob.glob(os.path.join(base_dir, \"CP034462_txt\", \"*.txt\"))\n",
    "wg_median_file = os.path.join(base_dir, \"WG_Median_Coverage_CAPITALIZED.csv\")\n",
    "apc_median_file = os.path.join(base_dir, \"WG_Median_Coverage_APC.csv\")  # Added APC median coverage file\n",
    "\n",
    "# Load annotated genome data\n",
    "genes_df = pd.read_csv(annotated_file)\n",
    "\n",
    "# Load whole-genome median coverage data\n",
    "wg_median_df = pd.read_csv(wg_median_file)\n",
    "wg_median_dict = dict(zip(wg_median_df[\"Strain\"], wg_median_df[\"Median Coverage\"]))  # {Strain: MedianCoverage}\n",
    "\n",
    "# Load APC median coverage data\n",
    "apc_median_df = pd.read_csv(apc_median_file)\n",
    "wg_median_dict.update(dict(zip(apc_median_df[\"Strain\"], apc_median_df[\"Median Coverage\"])))  # Merge APC data\n",
    "\n",
    "# Store processed data\n",
    "final_data = []\n",
    "\n",
    "# Process each strain's coverage file\n",
    "for file in coverage_files:\n",
    "    strain_name = os.path.basename(file).split(\"_\")[0].upper()  # Extract strain name\n",
    "    \n",
    "    # Load coverage data\n",
    "    coverage_df = pd.read_csv(file, sep=\"\\t\", header=None, names=[\"Chromosome\", \"Position\", \"Coverage\"])\n",
    "    coverage_dict = dict(zip(coverage_df[\"Position\"], coverage_df[\"Coverage\"]))  # {Position: Coverage}\n",
    "    \n",
    "    # Get the whole-genome median for this strain (default to 1 if missing to avoid division errors)\n",
    "    wg_median = wg_median_dict.get(strain_name, 1)\n",
    "    \n",
    "    # Process each gene\n",
    "    for _, gene in genes_df.iterrows():\n",
    "        gene_function = gene[\"Product\"]\n",
    "        gene_id = gene[\"Gene\"]\n",
    "        start_pos, end_pos = gene[\"Start\"], gene[\"End\"]\n",
    "        length = end_pos - start_pos  \n",
    "\n",
    "        # Extract coverage values within the gene range\n",
    "        coverage_values = [coverage_dict[pos] for pos in range(start_pos, end_pos + 1) if pos in coverage_dict]\n",
    "\n",
    "        # Calculate median coverage for the gene\n",
    "        median_coverage = round(np.median(coverage_values)) if coverage_values else None\n",
    "\n",
    "        # Calculate scaled coverage\n",
    "        scaled_coverage = round(median_coverage / wg_median, 3) if median_coverage is not None else None\n",
    "\n",
    "        # Store result\n",
    "        final_data.append([gene_id, gene_function, start_pos, end_pos, length, strain_name, scaled_coverage])\n",
    "\n",
    "# Convert to DataFrame\n",
    "final_df = pd.DataFrame(final_data, columns=[\"Gene ID\", \"Gene Function\", \"Start Position\", \"End Position\", \"Length\", \"Strain\", \"Scaled Coverage\"])\n",
    "\n",
    "# Pivot table to make strains as columns\n",
    "final_pivot = final_df.pivot(index=[\"Gene ID\", \"Gene Function\", \"Start Position\", \"End Position\", \"Length\"], columns=\"Strain\", values=\"Scaled Coverage\")\n",
    "\n",
    "# Reset column names for clarity\n",
    "final_pivot.columns = [f\"{strain}\" for strain in final_pivot.columns]\n",
    "final_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Save the final table\n",
    "output_file = os.path.join(base_dir, \"Table_CRC_allregions62.csv\")\n",
    "final_pivot.to_csv(output_file, index=False)\n",
    "\n",
    "# Display result\n",
    "print(final_pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f0e24-8b39-4836-87c2-0ca9aee39942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
