{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e237d659-dae7-4404-9c2e-02f043db0e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/evaedwards/Final-Year-Project/Datasets/TXT\n"
     ]
    }
   ],
   "source": [
    "# Redrafting CH58 analysis - Table_CRC_Ch58\n",
    "# Purpose here is to make a table with the following variables: Gene_ID, Gene_Function, \n",
    "# Start position, End position, Length, Mean coverage of the gene per strain\n",
    "\n",
    "# Step 1: Check working directory\n",
    "import os\n",
    "os.chdir('/Users/evaedwards/Final-Year-Project/Datasets/TXT')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edeb92a0-bbf5-4658-81ba-61c104f5ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved filtered data for matc2 to filtered_data/matc2_CH58short.txt\n",
      "Saved filtered data for WS4 to filtered_data/WS4_CH58short.txt\n",
      "Saved filtered data for jog19 to filtered_data/jog19_CH58short.txt\n",
      "Saved filtered data for WS5 to filtered_data/WS5_CH58short.txt\n",
      "Saved filtered data for matc3 to filtered_data/matc3_CH58short.txt\n",
      "Saved filtered data for matc1 to filtered_data/matc1_CH58short.txt\n",
      "Saved filtered data for WS7 to filtered_data/WS7_CH58short.txt\n",
      "Saved filtered data for WS6 to filtered_data/WS6_CH58short.txt\n",
      "Saved filtered data for matc4 to filtered_data/matc4_CH58short.txt\n",
      "Saved filtered data for WS2 to filtered_data/WS2_CH58short.txt\n",
      "Saved filtered data for WS3 to filtered_data/WS3_CH58short.txt\n",
      "Saved filtered data for WS1 to filtered_data/WS1_CH58short.txt\n",
      "Saved filtered data for jog20 to filtered_data/jog20_CH58short.txt\n",
      "Saved filtered data for jog21 to filtered_data/jog21_CH58short.txt\n",
      "Saved filtered data for mat22 to filtered_data/mat22_CH58short.txt\n",
      "Saved filtered data for PP16 to filtered_data/PP16_CH58short.txt\n",
      "Saved filtered data for jog1 to filtered_data/jog1_CH58short.txt\n",
      "Saved filtered data for jogc5 to filtered_data/jogc5_CH58short.txt\n",
      "Saved filtered data for jogc4 to filtered_data/jogc4_CH58short.txt\n",
      "Saved filtered data for PP17 to filtered_data/PP17_CH58short.txt\n",
      "Saved filtered data for mat23 to filtered_data/mat23_CH58short.txt\n",
      "Saved filtered data for mat21 to filtered_data/mat21_CH58short.txt\n",
      "Saved filtered data for PP15 to filtered_data/PP15_CH58short.txt\n",
      "Saved filtered data for jog2 to filtered_data/jog2_CH58short.txt\n",
      "Saved filtered data for NW13 to filtered_data/NW13_CH58short.txt\n",
      "Saved filtered data for jog3 to filtered_data/jog3_CH58short.txt\n",
      "Saved filtered data for PP14 to filtered_data/PP14_CH58short.txt\n",
      "Saved filtered data for mat20 to filtered_data/mat20_CH58short.txt\n",
      "Saved filtered data for mat24 to filtered_data/mat24_CH58short.txt\n",
      "Saved filtered data for mat18 to filtered_data/mat18_CH58short.txt\n",
      "Saved filtered data for PP10 to filtered_data/PP10_CH58short.txt\n",
      "Saved filtered data for jog7 to filtered_data/jog7_CH58short.txt\n",
      "Saved filtered data for NW16 to filtered_data/NW16_CH58short.txt\n",
      "Saved filtered data for NW17 to filtered_data/NW17_CH58short.txt\n",
      "Saved filtered data for jogc2 to filtered_data/jogc2_CH58short.txt\n",
      "Saved filtered data for jog6 to filtered_data/jog6_CH58short.txt\n",
      "Saved filtered data for PP11 to filtered_data/PP11_CH58short.txt\n",
      "Saved filtered data for mat19 to filtered_data/mat19_CH58short.txt\n",
      "Saved filtered data for PP8 to filtered_data/PP8_CH58short.txt\n",
      "Saved filtered data for PP13 to filtered_data/PP13_CH58short.txt\n",
      "Saved filtered data for NW15 to filtered_data/NW15_CH58short.txt\n",
      "Saved filtered data for NW29 to filtered_data/NW29_CH58short.txt\n",
      "Saved filtered data for NW28 to filtered_data/NW28_CH58short.txt\n",
      "Saved filtered data for jogc1 to filtered_data/jogc1_CH58short.txt\n",
      "Saved filtered data for NW14 to filtered_data/NW14_CH58short.txt\n",
      "Saved filtered data for jog5 to filtered_data/jog5_CH58short.txt\n",
      "Saved filtered data for PP12 to filtered_data/PP12_CH58short.txt\n",
      "Saved filtered data for PP9 to filtered_data/PP9_CH58short.txt\n",
      "Saved filtered data for mat17 to filtered_data/mat17_CH58short.txt\n",
      "Saved filtered data for PP4 to filtered_data/PP4_CH58short.txt\n",
      "Saved filtered data for PP23 to filtered_data/PP23_CH58short.txt\n",
      "Saved filtered data for jog8 to filtered_data/jog8_CH58short.txt\n",
      "Saved filtered data for NW19 to filtered_data/NW19_CH58short.txt\n",
      "Saved filtered data for NW31 to filtered_data/NW31_CH58short.txt\n",
      "Saved filtered data for NW25 to filtered_data/NW25_CH58short.txt\n",
      "Saved filtered data for NW24 to filtered_data/NW24_CH58short.txt\n",
      "Saved filtered data for NW30 to filtered_data/NW30_CH58short.txt\n",
      "Saved filtered data for NW18 to filtered_data/NW18_CH58short.txt\n",
      "Saved filtered data for PP22 to filtered_data/PP22_CH58short.txt\n",
      "Saved filtered data for PP5 to filtered_data/PP5_CH58short.txt\n",
      "Saved filtered data for mat16 to filtered_data/mat16_CH58short.txt\n",
      "Saved filtered data for mat14 to filtered_data/mat14_CH58short.txt\n",
      "Saved filtered data for PP7 to filtered_data/PP7_CH58short.txt\n",
      "Saved filtered data for PP20 to filtered_data/PP20_CH58short.txt\n",
      "Saved filtered data for NW26 to filtered_data/NW26_CH58short.txt\n",
      "Saved filtered data for NW32 to filtered_data/NW32_CH58short.txt\n",
      "Saved filtered data for NW33 to filtered_data/NW33_CH58short.txt\n",
      "Saved filtered data for NW27 to filtered_data/NW27_CH58short.txt\n",
      "Saved filtered data for PP21 to filtered_data/PP21_CH58short.txt\n",
      "Saved filtered data for PP6 to filtered_data/PP6_CH58short.txt\n",
      "Saved filtered data for mat15 to filtered_data/mat15_CH58short.txt\n",
      "Saved filtered data for PP25 to filtered_data/PP25_CH58short.txt\n",
      "Saved filtered data for PP19 to filtered_data/PP19_CH58short.txt\n",
      "Saved filtered data for NW23 to filtered_data/NW23_CH58short.txt\n",
      "Saved filtered data for NW36 to filtered_data/NW36_CH58short.txt\n",
      "Saved filtered data for NW22 to filtered_data/NW22_CH58short.txt\n",
      "Saved filtered data for PP18 to filtered_data/PP18_CH58short.txt\n",
      "Saved filtered data for PP3 to filtered_data/PP3_CH58short.txt\n",
      "Saved filtered data for PP24 to filtered_data/PP24_CH58short.txt\n",
      "Saved filtered data for PP1 to filtered_data/PP1_CH58short.txt\n",
      "Saved filtered data for NW34 to filtered_data/NW34_CH58short.txt\n",
      "Saved filtered data for NW20 to filtered_data/NW20_CH58short.txt\n",
      "Saved filtered data for NW21 to filtered_data/NW21_CH58short.txt\n",
      "Saved filtered data for NW35 to filtered_data/NW35_CH58short.txt\n",
      "Saved filtered data for mat13 to filtered_data/mat13_CH58short.txt\n",
      "Saved filtered data for jog10 to filtered_data/jog10_CH58short.txt\n",
      "Saved filtered data for jog11 to filtered_data/jog11_CH58short.txt\n",
      "Saved filtered data for jog13 to filtered_data/jog13_CH58short.txt\n",
      "Saved filtered data for jog12 to filtered_data/jog12_CH58short.txt\n",
      "Saved filtered data for WS11 to filtered_data/WS11_CH58short.txt\n",
      "Saved filtered data for WS10 to filtered_data/WS10_CH58short.txt\n",
      "Saved filtered data for WS8 to filtered_data/WS8_CH58short.txt\n",
      "Saved filtered data for WS12 to filtered_data/WS12_CH58short.txt\n",
      "Saved filtered data for jog15 to filtered_data/jog15_CH58short.txt\n",
      "Saved filtered data for jog14 to filtered_data/jog14_CH58short.txt\n",
      "Saved filtered data for WS9 to filtered_data/WS9_CH58short.txt\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Make shorter txt files for every strain for the region of interest for Ch58\n",
    "# we are doing every strain in order to compare between strains, we know the interesting ones\n",
    "# but having a comparison might be useful\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define input and output file paths\n",
    "input_files = glob.glob(\"CH58/*.txt\")  # Adjust to match your directory\n",
    "output_folder = \"filtered_data/\"  # Folder to store filtered files\n",
    "os.makedirs(output_folder, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "# Process each file\n",
    "for file in input_files:\n",
    "    strain_name = os.path.basename(file).replace(\".txt\", \"\")  # Extract strain name\n",
    "    \n",
    "    # Load the coverage data (assuming tab-separated format, no headers)\n",
    "    df = pd.read_csv(file, sep=\"\\t\", header=None, names=[\"Chromosome\", \"Position\", \"Coverage\"])\n",
    "    \n",
    "    # Filter only for CP034458 and positions 0-1,150,000 as this is the region of interest\n",
    "    df_filtered = df[(df[\"Chromosome\"] == \"CP034458\") & (df[\"Position\"] >= 0) & (df[\"Position\"] <= 1150000)]\n",
    "    \n",
    "    # Save the filtered data\n",
    "    output_file = os.path.join(output_folder, f\"{strain_name}_CH58short.txt\")\n",
    "    df_filtered.to_csv(output_file, sep=\"\\t\", index=False, header=False)\n",
    "    \n",
    "    print(f\"Saved filtered data for {strain_name} to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bde54663-20b6-423e-bf68-25d46d376529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved median coverages to filtered_data/Median_Coverage_CP58.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define input and output file paths\n",
    "input_files = glob.glob(\"CH58/*.txt\")  # Adjust to match your directory\n",
    "output_folder = \"filtered_data/\"  # Folder to store filtered files\n",
    "os.makedirs(output_folder, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "# Dictionary to store median coverages for each strain\n",
    "median_coverages = {}\n",
    "\n",
    "# Process each file\n",
    "for file in input_files:\n",
    "    strain_name = os.path.basename(file).replace(\".txt\", \"\")  # Extract strain name\n",
    "    \n",
    "    # Load the coverage data (assuming tab-separated format, no headers)\n",
    "    df = pd.read_csv(file, sep=\"\\t\", header=None, names=[\"Chromosome\", \"Position\", \"Coverage\"])\n",
    "    \n",
    "    # Filter only for CP034458 and positions 0-1,150,000 as this is the region of interest\n",
    "    df_filtered = df[(df[\"Chromosome\"] == \"CP034458\")]\n",
    "    \n",
    "    # Calculate median coverage for this strain\n",
    "    median_coverage = np.median(df_filtered[\"Coverage\"].dropna())  # Drop NaN values\n",
    "    median_coverages[strain_name] = round(median_coverage)  # Store rounded median\n",
    "\n",
    "    # Add a new column with median coverage (same for all rows in this strain's file)\n",
    "    df_filtered[\"Median Coverage\"] = round(median_coverage)  # Adds the median to each row\n",
    "    \n",
    "    # Save the filtered data with median coverage\n",
    "    output_file = os.path.join(output_folder, f\"{strain_name}_CH58.txt\")\n",
    "    df_filtered.to_csv(output_file, sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    print(f\"Saved filtered data for {strain_name} to {output_file}\")\n",
    "\n",
    "# Step 2: Create a separate CSV file with just the median coverages\n",
    "median_df = pd.DataFrame(list(median_coverages.items()), columns=[\"Strain\", \"Median Coverage\"])\n",
    "median_output_file = os.path.join(output_folder, \"Median_Coverage_CP58.csv\")\n",
    "median_df.to_csv(median_output_file, index=False)\n",
    "\n",
    "print(f\"Saved median coverages to {median_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "539da7be-218a-41de-b946-d00fb800bb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chromosome  Position  Coverage\n",
      "0   CP034458         1       152\n",
      "1   CP034458         2       213\n",
      "2   CP034458         3       216\n",
      "3   CP034458         4       227\n",
      "4   CP034458         5       243\n"
     ]
    }
   ],
   "source": [
    "# Checking it has worked\n",
    "df = pd.read_csv(\"filtered_data/WS4_CH58short.txt\", sep=\"\\t\", header=None, names=[\"Chromosome\", \"Position\", \"Coverage\"])\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67688f9d-05fb-4cc4-8deb-ace7cd75ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/Bio/SeqFeature.py:231: BiopythonDeprecationWarning: Please use .location.strand rather than .strand\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Gene                                            Product  \\\n",
      "0    MPUL0C00100  Hyphally regulated cell wall GPI-anchored prot...   \n",
      "1    MPUL0C00110    Hyphally regulated cell wall protein N-terminal   \n",
      "2    MPUL0C00120         small oligopeptide transporter, OPT family   \n",
      "3    MPUL0C00130         Pimeloyl-ACP methyl ester carboxylesterase   \n",
      "4        unknown                               hypothetical protein   \n",
      "..           ...                                                ...   \n",
      "390  MPUL0C04050                           DNA repair protein RAD50   \n",
      "391  MPUL0C04060                              C2H2-type zinc finger   \n",
      "392      unknown                               hypothetical protein   \n",
      "393  MPUL0C04080  DNA-directed RNA polymerase I and III subunit ...   \n",
      "394  MPUL0C04090               ATP-dependent RNA helicase DDX5/DBP2   \n",
      "\n",
      "     Start Position  End Position Strand  \n",
      "0             27681         38832      -  \n",
      "1             46148         50885      -  \n",
      "2             58599         61146      +  \n",
      "3             61437         62331      +  \n",
      "4             62794         63580      -  \n",
      "..              ...           ...    ...  \n",
      "390         1134154       1138060      -  \n",
      "391         1138706       1141223      -  \n",
      "392         1142783       1144268      -  \n",
      "393         1146170       1146542      -  \n",
      "394         1147831       1149298      +  \n",
      "\n",
      "[395 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Make the appropriate gbk file for the Chromosome ..58 and to the\n",
    "# correct position so we can then emalgamate\n",
    "\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "genbank_file = \"GCA_004217705.1_ASM421770v1_genomic-1.gbk\"\n",
    "record_id = \"CP034458.1\"\n",
    "\n",
    "# Initialize an empty list to store the gene information\n",
    "gene_data = []\n",
    "\n",
    "# Parse the GenBank file and find the specific record by ID\n",
    "with open(genbank_file, \"r\") as handle:\n",
    "    for record in SeqIO.parse(handle, \"genbank\"):\n",
    "        if record.id == record_id:\n",
    "            # Extract the CDS (coding region) annotations\n",
    "            for feature in record.features:\n",
    "                if feature.type == \"CDS\":\n",
    "                    start_position = int(feature.location.start)\n",
    "                    end_position = int(feature.location.end)\n",
    "\n",
    "                    # Filter to only include genes that start at â‰¤ 1,150,000\n",
    "                    if start_position <= 1150000:\n",
    "                        gene_name = feature.qualifiers.get(\"gene\", [\"unknown\"])[0]  # Get gene name (fallback to \"unknown\")\n",
    "                        product = feature.qualifiers.get(\"product\", [\"unknown\"])[0]  # Get product description\n",
    "                        strand = \"+\" if feature.strand == 1 else \"-\"  # Determine strand direction\n",
    "\n",
    "                        # Store gene information\n",
    "                        gene_info = {\n",
    "                            \"Gene\": gene_name,\n",
    "                            \"Product\": product,\n",
    "                            \"Start Position\": start_position,\n",
    "                            \"End Position\": end_position,\n",
    "                            \"Strand\": strand\n",
    "                        }\n",
    "                        gene_data.append(gene_info)\n",
    "\n",
    "# Create a DataFrame from the filtered gene data\n",
    "df = pd.DataFrame(gene_data)\n",
    "\n",
    "# Display the table\n",
    "print(df)\n",
    "\n",
    "# Save the filtered table to a CSV file\n",
    "df.to_csv(\"AnnotatedGenome_CP58_short.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "efad8fa4-3515-4cf5-b02c-c25f3fbb7b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Gene ID                                      Gene Function  \\\n",
      "0  MPUL0C00100  Hyphally regulated cell wall GPI-anchored prot...   \n",
      "1  MPUL0C00110    Hyphally regulated cell wall protein N-terminal   \n",
      "2  MPUL0C00120         small oligopeptide transporter, OPT family   \n",
      "3  MPUL0C00130         Pimeloyl-ACP methyl ester carboxylesterase   \n",
      "4  MPUL0C00150                                          mitofilin   \n",
      "\n",
      "   Start Position  End Position  Length  JOG1  JOG10  JOG11  JOG12  JOG13  \\\n",
      "0           27681         38832   11151  1059    604    715    393    599   \n",
      "1           46148         50885    4737   600    339    372    206    324   \n",
      "2           58599         61146    2547   334    205    217    127    195   \n",
      "3           61437         62331     894   324    194    204    112    186   \n",
      "4           66379         68122    1743   336    199    215    124    192   \n",
      "\n",
      "   ...  WS11  WS12  WS2  WS3  WS4   WS5   WS6  WS7  WS8  WS9  \n",
      "0  ...   450   622  584  424  690  1002  1082  557  453  564  \n",
      "1  ...   300   384  373  275  427   679   709  321  281  330  \n",
      "2  ...   171   223  207  158  256   377   393  202  161  199  \n",
      "3  ...   147   209  210  145  228   361   360  171  146  174  \n",
      "4  ...   168   215  209  148  245   373   396  180  161  188  \n",
      "\n",
      "[5 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 4\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "base_dir = \"/Users/evaedwards/Final-Year-Project/Datasets/TXT\"\n",
    "annotated_file = os.path.join(base_dir, \"AnnotatedGenome_CP58_short.csv\")\n",
    "coverage_files = glob.glob(os.path.join(base_dir, \"filtered_data\", \"*.txt\"))\n",
    "\n",
    "# Load annotated genome data\n",
    "genes_df = pd.read_csv(annotated_file)\n",
    "\n",
    "# Store processed data\n",
    "final_data = []\n",
    "\n",
    "# Process each strain's coverage file\n",
    "for file in coverage_files:\n",
    "    strain_name = os.path.basename(file).split(\"_\")[0].upper()  # Extract strain name\n",
    "    \n",
    "    # Load coverage data into a dictionary for fast lookup\n",
    "    coverage_df = pd.read_csv(file, sep=\"\\t\", header=None, names=[\"Chromosome\", \"Position\", \"Coverage\"])\n",
    "    coverage_dict = dict(zip(coverage_df[\"Position\"], coverage_df[\"Coverage\"]))  # {Position: Coverage}\n",
    "    \n",
    "    # Process each gene\n",
    "    for _, gene in genes_df.iterrows():\n",
    "        gene_function = gene[\"Product\"]  # Renaming Product to Gene Function\n",
    "        gene_id = gene[\"Gene\"]  # Assuming this column exists in the gbk annotation file\n",
    "        start_pos, end_pos = gene[\"Start Position\"], gene[\"End Position\"]\n",
    "        length = end_pos - start_pos  # Calculate length\n",
    "\n",
    "        # Extract coverage values within the gene range\n",
    "        coverage_values = [coverage_dict[pos] for pos in range(start_pos, end_pos + 1) if pos in coverage_dict]\n",
    "\n",
    "    \n",
    "        # Calculate mean coverage and round to nearest whole number for ease of display\n",
    "        mean_coverage = round(sum(coverage_values) / len(coverage_values)) if coverage_values else None\n",
    "        \n",
    "        # Store result\n",
    "        final_data.append([gene_id, gene_function, start_pos, end_pos, length, strain_name, mean_coverage])\n",
    "\n",
    "# Convert to DataFrame\n",
    "final_df = pd.DataFrame(final_data, columns=[\"Gene ID\", \"Gene Function\", \"Start Position\", \"End Position\", \"Length\", \"Strain\", \"Mean Coverage\"])\n",
    "\n",
    "# Pivot table to make strains as columns\n",
    "final_pivot = final_df.pivot(index=[\"Gene ID\", \"Gene Function\", \"Start Position\", \"End Position\", \"Length\"], columns=\"Strain\", values=\"Mean Coverage\")\n",
    "\n",
    "# Reset column names for clarity\n",
    "final_pivot.columns = [f\"{strain}\" for strain in final_pivot.columns]\n",
    "final_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Save the final table\n",
    "output_file = os.path.join(base_dir, \"Table_CRC_CH58_ALL.csv\")\n",
    "final_pivot.to_csv(output_file, index=False)\n",
    "\n",
    "# Display result\n",
    "print(final_pivot.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1aabd535-7c39-45fb-8d24-ba1e0c5c7435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CH58/matc2.txt', 'CH58/WS4.txt', 'CH58/jog19.txt', 'CH58/WS5.txt', 'CH58/matc3.txt', 'CH58/matc1.txt', 'CH58/WS7.txt', 'CH58/WS6.txt', 'CH58/matc4.txt', 'CH58/WS2.txt', 'CH58/WS3.txt', 'CH58/WS1.txt', 'CH58/jog20.txt', 'CH58/jog21.txt', 'CH58/mat22.txt', 'CH58/PP16.txt', 'CH58/jog1.txt', 'CH58/jogc5.txt', 'CH58/jogc4.txt', 'CH58/PP17.txt', 'CH58/mat23.txt', 'CH58/mat21.txt', 'CH58/PP15.txt', 'CH58/jog2.txt', 'CH58/NW13.txt', 'CH58/jog3.txt', 'CH58/PP14.txt', 'CH58/mat20.txt', 'CH58/mat24.txt', 'CH58/mat18.txt', 'CH58/PP10.txt', 'CH58/jog7.txt', 'CH58/NW16.txt', 'CH58/NW17.txt', 'CH58/jogc2.txt', 'CH58/jog6.txt', 'CH58/PP11.txt', 'CH58/mat19.txt', 'CH58/PP8.txt', 'CH58/PP13.txt', 'CH58/NW15.txt', 'CH58/NW29.txt', 'CH58/NW28.txt', 'CH58/jogc1.txt', 'CH58/NW14.txt', 'CH58/jog5.txt', 'CH58/PP12.txt', 'CH58/PP9.txt', 'CH58/mat17.txt', 'CH58/PP4.txt', 'CH58/PP23.txt', 'CH58/jog8.txt', 'CH58/NW19.txt', 'CH58/NW31.txt', 'CH58/NW25.txt', 'CH58/NW24.txt', 'CH58/NW30.txt', 'CH58/NW18.txt', 'CH58/PP22.txt', 'CH58/PP5.txt', 'CH58/mat16.txt', 'CH58/mat14.txt', 'CH58/PP7.txt', 'CH58/PP20.txt', 'CH58/NW26.txt', 'CH58/NW32.txt', 'CH58/NW33.txt', 'CH58/NW27.txt', 'CH58/PP21.txt', 'CH58/PP6.txt', 'CH58/mat15.txt', 'CH58/PP25.txt', 'CH58/PP19.txt', 'CH58/NW23.txt', 'CH58/NW36.txt', 'CH58/NW22.txt', 'CH58/PP18.txt', 'CH58/PP3.txt', 'CH58/PP24.txt', 'CH58/PP1.txt', 'CH58/NW34.txt', 'CH58/NW20.txt', 'CH58/NW21.txt', 'CH58/NW35.txt', 'CH58/mat13.txt', 'CH58/jog10.txt', 'CH58/jog11.txt', 'CH58/jog13.txt', 'CH58/jog12.txt', 'CH58/WS11.txt', 'CH58/WS10.txt', 'CH58/WS8.txt', 'CH58/WS12.txt', 'CH58/jog15.txt', 'CH58/jog14.txt', 'CH58/WS9.txt']\n",
      "Saved median coverages to filtered_data/Median_Coverage_CP58.csv\n"
     ]
    }
   ],
   "source": [
    "# Work out median coverage per starin to add it to file in order to standardize\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.chdir('/Users/evaedwards/Final-Year-Project/Datasets/TXT')\n",
    "\n",
    "# Define input files and output file\n",
    "input_files = glob.glob(\"CH58/*.txt\")  # Adjust to match your directory\n",
    "output_file = \"filtered_data/Median_Coverage_CP58.csv\"  # CSV to store results\n",
    "print(input_files)\n",
    "\n",
    "# Dictionary to store median coverages\n",
    "median_coverages = {}\n",
    "\n",
    "# Process each file\n",
    "for file in input_files:\n",
    "    strain_name = os.path.basename(file).replace(\".txt\", \"\")  # Extract strain name\n",
    "\n",
    "    # Load the coverage data (assuming tab-separated format, no headers)\n",
    "    df = pd.read_csv(file, sep=\"\\t\", header=None, names=[\"Chromosome\", \"Position\", \"Coverage\"])\n",
    "\n",
    "    # Filter only for CP034458\n",
    "    df_filtered = df[df[\"Chromosome\"] == \"CP034458\"]\n",
    "\n",
    "    # Calculate median coverage for CP034458\n",
    "    median_coverage = np.median(df_filtered[\"Coverage\"].dropna())  # Drop NaN values\n",
    "    median_coverages[strain_name] = round(median_coverage)  # Store rounded median\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "median_df = pd.DataFrame(list(median_coverages.items()), columns=[\"Strain\", \"Median Coverage\"])\n",
    "\n",
    "# Save to CSV\n",
    "median_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Saved median coverages to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
